{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image, plotting, datasets, masking\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.signal import clean\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.signal import correlate, correlation_lags\n",
    "from scipy.stats import mode, pearsonr, spearmanr, chi2_contingency, chisquare\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "import networkx as nx\n",
    "from math import log10\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from glob import glob \n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time_series(fname, gsr=False, to_csv=None):\n",
    "    img = image.load_img(fname) \n",
    "\n",
    "#     ref = type('',(object,),\\\n",
    "#        {\"maps\": 'data/templates_GMatlas_GMatlas.nii.gz',\\\n",
    "#         \"labels\":[line.strip() for line in open('data/templates_GMatlas_GMatlas.txt').readlines()]})()\n",
    "\n",
    "    ref = type('',(object,),\\\n",
    "       {\"maps\": 'data/46_rois.nii.gz',\\\n",
    "        \"labels\":[line.strip() for line in open('data/46_rois.txt').readlines()]})()\n",
    "\n",
    "    parcellation = image.load_img(ref.maps)\n",
    "    masker = NiftiLabelsMasker(labels_img=parcellation, standardize=True,\\\n",
    "                               memory='nilearn_cache', high_pass=0.01, low_pass=0.1, t_r=0.735)\n",
    "    time_series = masker.fit_transform(img)\n",
    "    \n",
    "    if gsr:\n",
    "        gs = np.mean(time_series, axis=1)\n",
    "        time_series = clean(time_series, confounds=[gs]);\n",
    "        \n",
    "    time_series = pd.DataFrame(time_series)\n",
    "    time_series.columns = ref.labels\n",
    "    time_series.index = np.array(range(1, len(time_series)+1))*0.735\n",
    "    \n",
    "    if to_csv!=None:\n",
    "        time_series.to_csv(to_csv)\n",
    "        \n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(fname, time_series):\n",
    "    with open(fname, encoding='utf-16') as f:\n",
    "        content = f.read() \n",
    " \n",
    "    header, content = content.split('*** Header End ***') \n",
    "    frames = content.split('*** LogFrame Start ***') \n",
    "    dicts = [] \n",
    "    for frame in frames[1:-1]: \n",
    "       lines = frame.split('\\n') \n",
    "       line_dict = {} \n",
    "       for line in lines[1:-3]: \n",
    "          k, v = line.split(': ') \n",
    "          line_dict[k] = v \n",
    "       dicts.append(line_dict) \n",
    "    \n",
    "    frames = pd.DataFrame(dicts) \n",
    "    frames['time_ms'] = frames.apply(lambda x: x['ExperimenterWindow.OnsetTime']\\\n",
    "          if math.isnan(float(x['ExperimenterWindow.OnsetTime']))==False else x['EightSecFix.OnsetTime'], axis=1)\n",
    "    frames['time_sec'] = ((frames['time_ms'].astype(int)/1000))\n",
    "    frames['time_sec'] = frames['time_sec']-frames['time_sec'].min()+0.735\n",
    "    \n",
    "    frames_cats = frames[frames['Procedure']!='TrialsPROC']\n",
    "    ys = np.array([frames_cats.loc[frames_cats['time_sec']<=idx, 'Procedure'].iloc[-1].replace('PromptPROC', '')\\\n",
    "          for idx in time_series.index])\n",
    "    \n",
    "    return frames, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import view_connectome, view_markers\n",
    "from nilearn.image import coord_transform\n",
    "\n",
    "def get_mni_coords(img_fname, labels):\n",
    "    img = image.load_img(img_fname)\n",
    "    mni = datasets.load_mni152_brain_mask()\n",
    "\n",
    "    img_resampled = image.resample_to_img(img, mni, interpolation='nearest')\n",
    "\n",
    "    img_data = img_resampled.get_fdata()\n",
    "    rois = set(img_data.flatten())\n",
    "\n",
    "    mean_coord = {}\n",
    "    for roi in rois:\n",
    "        if roi==0:\n",
    "            continue\n",
    "        x, y, z =  np.mean(np.where(img_data==roi), axis=1)\n",
    "        x, y, z = coord_transform(x, y, z, affine=mni.affine)\n",
    "        mean_coord[roi] = [x, y, z]\n",
    "    mean_coord = pd.DataFrame(mean_coord).transpose()\n",
    "    mean_coord.columns = ['x','y', 'z']\n",
    "    mean_coord.index = labels\n",
    "    return mean_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stat_map(conns):\n",
    "    atlas = image.load_img(ref.maps)\n",
    "    circuit = atlas.get_fdata().copy()\n",
    "    rois  = ref.labels\n",
    "    for roi_idx in range(1, len(rois)+1):\n",
    "        roi = rois[roi_idx-1]\n",
    "        circuit[circuit==roi_idx] = conns[roi] #conns[roi] if conns[roi]==1 else (0.5 if conns[roi] >0.78 else 0)\n",
    "    return image.new_img_like(atlas, circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_timeseries(time_series, window, ys, kind='covariance'):\n",
    "    \n",
    "    rois         = time_series.columns\n",
    "\n",
    "    chunks       = [time_series.iloc[ts:ts+window] for ts in range(len(time_series))[::window] if len(time_series.iloc[ts:ts+window])==window]\n",
    "    window_ys    = [mode(ys[ts:ts+window])[0][0] for ts in range(len(time_series))[::window]   if len(time_series.iloc[ts:ts+window])==window]\n",
    "    window_times = [int(np.mean(chunk.index)) for chunk in chunks]\n",
    "\n",
    "    conn         = ConnectivityMeasure(kind=kind)\n",
    "    corrs        = conn.fit_transform([chunk.values for chunk in chunks])\n",
    "    corrs        = [pd.DataFrame(corr, index=rois, columns=rois) for corr in corrs]\n",
    "    \n",
    "    return chunks, corrs, window_ys, window_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crosscorrs_cov(timeseries, zerolag=True):\n",
    "    rois       = timeseries.columns\n",
    "    lags       = correlation_lags(len(timeseries), len(timeseries))\n",
    "    crosscorrs = timeseries.corr(lambda x, y: np.max(abs(correlate(x, y))))\n",
    "    crosslags  = timeseries.corr(lambda x, y: lags[np.argmax(abs(correlate(x,y)))])\n",
    "    \n",
    "    crosslags = pd.DataFrame(index=rois, columns=rois)\n",
    "    for roi_x in timeseries:\n",
    "        for roi_y in timeseries:\n",
    "            if zerolag:\n",
    "                crosslags.loc[roi_x, roi_y] =\\\n",
    "                    lags[np.argmax(abs(correlate(timeseries[roi_x],timeseries[roi_y])))]\n",
    "            else:\n",
    "                lags_sorted = lags[np.argsort(abs(correlate(timeseries[roi_x],timeseries[roi_y])))]\n",
    "                crosslags.loc[roi_x, roi_y] = lags_sorted[-1] if lags_sorted[-1]!=0 else lags_sorted[-2]\n",
    "    \n",
    "    covs       = pd.DataFrame(np.cov(timeseries.transpose()), columns=timeseries.columns, index=timeseries.columns)\n",
    "    #pearson    = timeseries.corr(lambda x, y: pearsonr(x, y)[0])\n",
    "    #pvals      = timeseries.corr(lambda x, y: pearsonr(x, y)[1])\n",
    "\n",
    "    return crosscorrs, crosslags, covs#pearson, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tvfcn_crosscorrs_covs_full(chunks, zerolag=True):\n",
    "    X, Y         = chunks[0].columns, chunks[0].columns\n",
    "    tvfcn_edges  = ['%s > %s' % (x, y) for x in X for y in Y]\n",
    "    \n",
    "    corrs, lags, covs = [], [], []\n",
    "    for chunk in chunks:\n",
    "        a, b, c = compute_crosscorrs_cov(chunk, zerolag)\n",
    "        corrs.append(a.values.flatten())\n",
    "        lags.append(b.values.flatten())\n",
    "        covs.append(c.values.flatten())\n",
    "        \n",
    "    corrs, lags, covs  = pd.DataFrame(corrs).transpose(), pd.DataFrame(lags).transpose(), \\\n",
    "                         pd.DataFrame(covs).transpose()\n",
    "    corrs.index = lags.index = covs.index = tvfcn_edges\n",
    "    covs = covs.round(1)\n",
    "    return corrs, lags, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tvfcn_crosscorrs_covs(chunks):\n",
    "    indices      = np.tril_indices(chunks[0].shape[1], k=-1)\n",
    "    X, Y         = chunks[0].columns[indices[0]], chunks[0].columns[indices[1]]\n",
    "    tvfcn_edges  = ['%s > %s' % (x, y) for (x, y) in zip(X, Y)]\n",
    "    \n",
    "    corrs, lags, covs = [], [], []\n",
    "    for chunk in tqdm(chunks):\n",
    "        a, b, c = compute_crosscorrs_cov(chunk)\n",
    "        corrs.append(a.values[indices])\n",
    "        lags.append(b.values[indices])\n",
    "        covs.append(c.values[indices])\n",
    "        \n",
    "    corrs, lags, covs  = pd.DataFrame(corrs).transpose(), pd.DataFrame(lags).transpose(), \\\n",
    "                         pd.DataFrame(covs).transpose()\n",
    "    corrs.index = lags.index = covs.index = tvfcn_edges\n",
    "    return corrs, lags, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crosscorrs_forloop(timeseries):\n",
    "    rois = timeseries.columns\n",
    "    results = []\n",
    "    \n",
    "    lags = correlation_lags(len(timeseries), len(timeseries))\n",
    "        \n",
    "    for roi_x in rois:\n",
    "        for roi_y in rois:\n",
    "            crosscorrs  = correlate(timeseries[roi_x], timeseries[roi_y])\n",
    "            maxcross    = np.max(abs(crosscorrs))\n",
    "            maxcorr_lag = lags[np.argmax(abs(crosscorrs))]\n",
    "            pearson, pval = pearsonr(timeseries[roi_x], np.roll(timeseries[roi_y], maxcorr_lag))\n",
    "            results.append((roi_x, roi_y, maxcross, maxcorr_lag, pearson, pval))\n",
    "        \n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = ['roi_x', 'roi_y', 'cross', 'lag', 'pearsonr', 'pval']\n",
    "\n",
    "    cross   = results.pivot(index='roi_x', columns='roi_y', values='cross')\n",
    "    lags    = results.pivot(index='roi_x', columns='roi_y', values='lag')\n",
    "    pearson = results.pivot(index='roi_x', columns='roi_y', values='pearsonr')\n",
    "    pvals   = results.pivot(index='roi_x', columns='roi_y', values='pval')\n",
    "\n",
    "    return cross, lags, pearson, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tvfcn_crosscorrs(chunks):\n",
    "    indices      = np.tril_indices(chunks[0].shape[1], k=-1)\n",
    "    X, Y         = chunks[0].columns[indices[0]], chunks[0].columns[indices[1]]\n",
    "    tvfcn_edges  = ['%s > %s' % (x, y) for (x, y) in zip(X, Y)]\n",
    "    \n",
    "    corrs, lags, pearson, pvals = [], [], [], []\n",
    "    for chunk in chunks:\n",
    "        a, b, c, d = compute_crosscorrs_forloop(chunk)\n",
    "        corrs.append(a.values[indices])\n",
    "        lags.append(b.values[indices])\n",
    "        pearson.append(c.values[indices])\n",
    "        pvals.append(d.values[indices])\n",
    "        \n",
    "    corrs, lags, pearson, pvals =   pd.DataFrame(corrs).transpose(),   pd.DataFrame(lags).transpose(), \\\n",
    "                                    pd.DataFrame(pearson).transpose(), pd.DataFrame(pvals).transpose()\n",
    "    corrs.index = lags.index = pearson.index = pvals.index = tvfcn_edges\n",
    "    return corrs, lags, pearson, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def compute_instabilities_from_ts(time_series, win_len=30, nonoverlapping=True):\n",
    "    chunks = []\n",
    "    steps = range(len(time_series))\n",
    "    steps = steps[::win_len] if nonoverlapping else steps\n",
    "    for step in steps:\n",
    "        chunk = time_series[step:step+win_len]\n",
    "        if len(chunk)<win_len:\n",
    "            continue\n",
    "        chunks.append(chunk.values)\n",
    "\n",
    "    conn = ConnectivityMeasure(kind='correlation')\n",
    "    corrs = conn.fit_transform(chunks)\n",
    "\n",
    "    instabilities = []\n",
    "    for idx in range(1, len(corrs)):\n",
    "        instabilities.append(norm(corrs[idx]-corrs[idx-1])/corrs[0].shape[0])\n",
    "    \n",
    "    return instabilities\n",
    "\n",
    "def compute_instabilities(corrs):\n",
    "     return np.array([norm(corrs[idx]-corrs[idx-1]) for idx in range(1, len(corrs))])/np.prod(corrs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tvfcn(corrs):\n",
    "    indices      = np.tril_indices(len(corrs[0]), k=-1)\n",
    "\n",
    "    tvfcn        = []\n",
    "    for corr in corrs:\n",
    "        tvfcn.append(corr.values[indices])\n",
    "\n",
    "    X, Y         = corrs[0].columns[indices[0]], corrs[0].columns[indices[1]]\n",
    "    tvfcn_edges  = ['%s > %s' % (x, y) for (x, y) in zip(X, Y)]\n",
    "    tvfcn = pd.DataFrame(tvfcn).transpose()\n",
    "    tvfcn.index = tvfcn_edges\n",
    "    return tvfcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, optimal_leaf_ordering, leaves_list, fcluster, dendrogram\n",
    "\n",
    "def reorder_columns_by_dist(dist_matrix):\n",
    "    linkage_matrix  = linkage(dist_matrix, method='average')\n",
    "    ordered_linkage = optimal_leaf_ordering(linkage_matrix, dist_matrix)\n",
    "    index           = leaves_list(ordered_linkage)\n",
    "    cols            = [dist_matrix.index[idx] for idx in index]\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matrix(matrix, method='average', thresh=0.7, toplot=False, roi_to_highlight='Amygdala', metric='euclidean'):\n",
    "    rois            = np.array(matrix.index)\n",
    "    linkage_matrix  = linkage(matrix, method=method, metric=metric)\n",
    "    ordered_linkage = optimal_leaf_ordering(linkage_matrix, matrix)\n",
    "    index           = leaves_list(ordered_linkage)\n",
    "    reorderedcols   = [rois[idx] for idx in index]\n",
    "    \n",
    "    color_thresh    = thresh = thresh*max(linkage_matrix[:,2])\n",
    "\n",
    "    clusters        = fcluster(linkage_matrix, t=color_thresh, criterion='distance')\n",
    "    \n",
    "    dn = None\n",
    "    if toplot:\n",
    "        dn = dendrogram(linkage_matrix, labels=list(rois), orientation='right', leaf_font_size=10)#, color_threshold=thresh)        \n",
    "        [x.set_color(\"red\") for x in plt.gca().get_yticklabels() if (x.get_text()==roi_to_highlight)];\n",
    "        \n",
    "    return linkage_matrix, color_thresh, reorderedcols, clusters, dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_edges_tvfcn(edges, rois, clusters):\n",
    "    edge_order = []\n",
    "    for edge in tqdm(edges):\n",
    "        a, b      = edge.split(' > ')\n",
    "        cluster_a = clusters[rois==a][0]\n",
    "        cluster_b = clusters[rois==b][0]\n",
    "        edge_order.append((edge, cluster_a if (cluster_a==cluster_b) else cluster_a+0.5))\n",
    "\n",
    "    edge_order    = pd.DataFrame(edge_order).sort_values(1)\n",
    "    #edges_ordered = edge_order[0].values, edge_order[1].values\n",
    "\n",
    "    return edge_order[0].values, edge_order[1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "def compute_stimulus_pval_edges(edge, stimulus, method=ttest_rel, alternative='greater'):\n",
    "    face_edge = edge[np.array(stimulus)=='Face']\n",
    "    shape_edge = edge[np.array(stimulus)=='Shape']\n",
    "\n",
    "    min_len = min(len(face_edge), len(shape_edge))\n",
    "    return method(face_edge[:min_len], shape_edge[:min_len], alternative=alternative)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_types(covs, lags, rois, window_ys):\n",
    "    indices = np.tril_indices(len(rois), k=-1)\n",
    "    X, Y = indices\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    rel_dict = {0:'-', 1:'x <> y', -1: 'x <<>> y', 2:'x > y', -2: 'x >> y', 3: 'x < y', -3:'x << y'}\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        covs_xy = covs.loc['%s > %s' % (rois[x], rois[y])]#rois[x]+' > '+rois[y]\n",
    "        lags_xy = lags.loc['%s > %s' % (rois[x], rois[y])]#rois[x]+' > '+rois[y]\n",
    "        rel = pd.DataFrame(pd.concat([covs_xy, lags_xy], axis=1))\n",
    "        rel.columns = ['cov', 'lag']\n",
    "        rel.loc[rel['cov']==0, 'rel'] = 0\n",
    "        rel.loc[(rel['cov']>0) & (rel['lag']==0), 'rel'] = 1\n",
    "        rel.loc[(rel['cov']<0) & (rel['lag']==0), 'rel'] = -1\n",
    "        rel.loc[(rel['cov']>0) & (rel['lag']<0), 'rel'] = 2\n",
    "        rel.loc[(rel['cov']<0) & (rel['lag']<0), 'rel'] = -2\n",
    "        rel.loc[(rel['cov']>0) & (rel['lag']>0), 'rel'] = 3\n",
    "        rel.loc[(rel['cov']<0) & (rel['lag']>0), 'rel'] = -3\n",
    "        results[\"%s, %s\" % (rois[x], rois[y])] = rel['rel']\n",
    "    # results.replace(rel_dict)\n",
    "    # print(results)\n",
    "    results = results.transpose()\n",
    "    results.columns   = window_ys\n",
    "    return results, rel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_edge_types(edge_types, rel_dict):\n",
    "    counts = edge_types.apply(lambda x: pd.Series([round(100*sum(x==edge_type)/len(x)) for edge_type in range(-3, 4)]), axis=1)\n",
    "    #counts = edge_types.apply(lambda x: pd.Series([sum(x==edge_type) for edge_type in range(-3, 4)]), axis=1)\n",
    "    counts.columns = [rel_dict[x] for x in range(-3, 4)]\n",
    "    counts = counts[['-', 'x > y', 'x >> y', 'x < y','x << y' ]]# 'x <> y', 'x <<>> y',\n",
    "    #counts = counts.replace({0:1})\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_labels(x):\n",
    "    nonzeros = x[x.index!='-'][x>0].index\n",
    "    if len(nonzeros)<1:\n",
    "        return None, None\n",
    "    elif len(nonzeros)==1:\n",
    "        arrowhead = (nonzeros[0].split(' ')[1])\n",
    "        if arrowhead=='>':\n",
    "            return 'excitatory', '>'\n",
    "        elif arrowhead=='<':\n",
    "            return 'excitatory', '<'\n",
    "        elif arrowhead=='>>':\n",
    "            return 'inhibitory', '>'\n",
    "        elif arrowhead=='<<':\n",
    "            return 'inhibitory', '<'\n",
    "    else:\n",
    "        a, b = nonzeros[0].split(' ')[1], nonzeros[1].split(' ')[1]\n",
    "        if   len(set(['>', '<']).intersection(set([a, b])))==2:\n",
    "            return '+ve feedback loop', None\n",
    "        elif len(set(['>', '<<']).intersection(set([a, b])))==2:\n",
    "            return '-ve feedback loop', None\n",
    "        elif len(set(['>>', '<<']).intersection(set([a, b])))==2:\n",
    "            return '+ve feedback loop', None\n",
    "        elif len(set(['>>', '<']).intersection(set([a, b])))==2:\n",
    "            return '-ve feedback loop', None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(edges):\n",
    "    G = nx.DiGraph()\n",
    "    for rois, (nature, arrow, pval) in edges.iterrows():\n",
    "        a, b = rois.split(', ')\n",
    "        w = -log10(pval)\n",
    "        if nature==None:\n",
    "            continue\n",
    "        if 'loop' in nature:\n",
    "            G.add_edge(a, b, label=nature, weight=w)\n",
    "            G.add_edge(b, a, label=nature, weight=w)\n",
    "        elif arrow=='>':\n",
    "            G.add_edge(a, b, label=nature, weight=w)\n",
    "        elif arrow=='<':\n",
    "            G.add_edge(b, a, label=nature, weight=w)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot windowed timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def plot_windowed_timeseries(chunks, ys, x_indices):\n",
    "    # Plot time series\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.xticks(x_indices[::2], ys[::2], rotation=90);\n",
    "    [plt.plot(chunk, linewidth=3, color='#%06X' % randint(0, 0xFFFFFF)) for chunk in chunks];\n",
    "    #print(set([len(chunk) for chunk in chunks]));\n",
    "    plt.xlabel('Stimulus');\n",
    "    plt.xlabel('BOLD');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot windowed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_windowed_matrices(corrs, window_ys):\n",
    "    # Plot matrices\n",
    "    fig, axs = plt.subplots(1, len(corrs), figsize=(20, 4));\n",
    "    for idx, corr in enumerate(corrs):\n",
    "        axs[idx].imshow(corr, cmap='bwr', vmin=-1, vmax=1);\n",
    "        axs[idx].set_xticks([]);\n",
    "        axs[idx].set_yticks([]);\n",
    "        axs[idx].set_xlabel(window_ys[idx], rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tvfcn(data, xticks, reorder=True, figsize=(20, 4), vmin=None, vmax=None, cmap='bwr', fontsize=10, xlabel=None, ylabel=None, xrotation=90):\n",
    "    if reorder:\n",
    "        data = data.loc[reorder_columns_by_dist(data)]\n",
    "\n",
    "    plt.figure(figsize=figsize);\n",
    "    plt.imshow(data, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax);\n",
    "    plt.colorbar();\n",
    "    plt.xticks(range(len(xticks)), xticks, rotation=xrotation, fontsize=fontsize);\n",
    "    plt.xlabel(xlabel);\n",
    "    plt.ylabel(ylabel);\n",
    "    plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G, layout=None, pos=None, edge_weights=False, figsize=(20, 10), xlim=None, ylim=None, edgecolor='blue', fontcolor='red'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if pos==None:\n",
    "        pos = layout(G)\n",
    "\n",
    "    weights = [G[u][v]['weight'] for u,v in G.edges()] if edge_weights else None\n",
    "    nx.draw(G, with_labels=True, pos=pos, edge_color=edgecolor, arrowsize=50, font_size=20, node_size=0, width=weights)\n",
    "\n",
    "    edge_labels_formatted = {(e[0], e[1]):G.get_edge_data(e[0], e[1])['label'] for e in list(G.edges())}\n",
    "    nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels_formatted, font_color=fontcolor, font_size=10);\n",
    "    \n",
    "    # plt.ylim(-50, 70);\n",
    "    # plt.xlim(-110, 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gc(chunk):\n",
    "    rois    = chunk.columns\n",
    "    gclags  = pd.DataFrame(index=rois, columns=rois)\n",
    "    gcstats = pd.DataFrame(index=rois, columns=rois)\n",
    "    gcpvals = pd.DataFrame(index=rois, columns=rois)\n",
    "    corrs   = pd.DataFrame(index=rois, columns=rois)\n",
    "    for roi_x in chunk:\n",
    "        for roi_y in chunk:\n",
    "            chunk_xy  = np.array([chunk[roi_y], chunk[roi_x]]).transpose();\n",
    "            result_xy = gcresult_to_df(grangercausalitytests(chunk_xy, maxlag=2, verbose=False))\n",
    "            lag, pval = result_xy['ssr_ftest_pval'].idxmin(), result_xy['ssr_ftest_pval'].min()\n",
    "            stat      = result_xy.loc[lag, 'ssr_ftest_stat']\n",
    "            gclags.loc[roi_x, roi_y]  = lag\n",
    "            gcpvals.loc[roi_x, roi_y] = pval\n",
    "            gcstats.loc[roi_x, roi_y] = stat\n",
    "            corrs.loc[roi_x, roi_y]   = pearsonr(chunk[roi_y], chunk[roi_x])[0]\n",
    "    return gcstats, gclags, gcpvals, corrs\n",
    "\n",
    "def compute_tvfcn_gc(chunks, window=10):\n",
    "    X, Y         = chunks[0].columns, chunks[0].columns\n",
    "    tvfcn_edges  = ['%s > %s' % (x, y) for x in X for y in Y]\n",
    "    stats, lags, pvals, corrs = [], [], [], []\n",
    "    for chunk in tqdm(chunks):\n",
    "        if len(chunk)<window:\n",
    "            continue\n",
    "        a, b, c, d = compute_gc(chunk)\n",
    "        stats.append(a.values.flatten())\n",
    "        lags.append(b.values.flatten())\n",
    "        pvals.append(c.values.flatten())\n",
    "        corrs.append(d.values.flatten())\n",
    "    stats, lags, pvals, corrs  = \\\n",
    "        pd.DataFrame(stats).transpose(),\\\n",
    "        pd.DataFrame(lags).transpose(), \\\n",
    "        pd.DataFrame(pvals).transpose(),\\\n",
    "        pd.DataFrame(corrs).transpose()\n",
    "\n",
    "    stats.index = lags.index = pvals.index = corrs.index = tvfcn_edges\n",
    "    return stats, lags, pvals, corrs\n",
    "\n",
    "def get_edge_types_gc(stats, lags, pvals, corrs, rois, window_ys):\n",
    "    indices = np.tril_indices(len(rois), k=-1)\n",
    "    X, Y = indices\n",
    "    results = pd.DataFrame()\n",
    "    rel_dict = {0:'-', 1:'x <> y', -1: 'x <<>> y', 2:'x > y', -2: 'x >> y', 3: 'x < y', -3:'x << y'}\n",
    "    for x, y in zip(X, Y):\n",
    "        \n",
    "        xy_pval = pvals.loc[rois[x]+' > '+rois[y]] \n",
    "        yx_pval = pvals.loc[rois[y]+' > '+rois[x]]\n",
    "        \n",
    "        xy_corr = corrs.loc[rois[x]+' > '+rois[y]]\n",
    "        yx_corr = corrs.loc[rois[y]+' > '+rois[x]]\n",
    "        \n",
    "        rel = pd.DataFrame(pd.concat([xy_pval, yx_pval, xy_corr, yx_corr], axis=1))\n",
    "        rel.columns = ['xy_pval', 'yx_pval', 'xy_corr', 'yx_corr']\n",
    "        rel['rel'] = None\n",
    "        rel.loc[(rel['xy_pval']>rel['yx_pval']) & (rel['xy_corr']>0), 'rel'] = 'x < y'\n",
    "        rel.loc[(rel['xy_pval']>rel['yx_pval']) & (rel['xy_corr']<0), 'rel'] = 'x << y'\n",
    "        rel.loc[(rel['xy_pval']<rel['yx_pval']) & (rel['xy_corr']>0), 'rel'] = 'x > y'\n",
    "        rel.loc[(rel['xy_pval']<rel['yx_pval']) & (rel['xy_corr']<0), 'rel'] = 'x >> y'\n",
    "        rel.loc[(rel['xy_pval']>(0.001/np.prod(pvals.shape))) &\\\n",
    "                (rel['yx_pval']>(0.001/np.prod(pvals.shape))), 'rel'] = '-'\n",
    "        \n",
    "        results[\"%s, %s\" % (rois[x], rois[y])] = rel['rel']\n",
    "    results = results.transpose()\n",
    "    results.columns   = window_ys#[:-1]\n",
    "    rel_dict = {rel_dict[k]:k for k in rel_dict}\n",
    "    return results, rel_dict\n",
    "\n",
    "# def gcresult_to_df(result):\n",
    "#     lags = result.keys()\n",
    "#     tests = result[key][0].keys()\n",
    "#     b = []\n",
    "#     for lag in lags:\n",
    "#         tmp = []\n",
    "#         for test in tests:\n",
    "#             tmp = tmp + list(result[lag][0][test][:2])\n",
    "#         b.append(tmp)\n",
    "\n",
    "#     df = pd.DataFrame(b)\n",
    "#     df.columns = np.concatenate([[test+'_stat', test+'_pval']for test in tests])\n",
    "#     df.index = lags\n",
    "#     return df\n",
    "\n",
    "def gcresult_to_df(result):\n",
    "    lags = result.keys()\n",
    "    tests = result[1][0].keys()\n",
    "    b = []\n",
    "    for lag in lags:\n",
    "        tmp = []\n",
    "        for test in tests:\n",
    "            tmp = tmp + list(result[lag][0][test][:2])+[result[lag][1][0].params[-1], result[lag][1][1].params[-1]]\n",
    "        b.append(tmp)\n",
    "\n",
    "    df = pd.DataFrame(b)\n",
    "    df.columns = list(np.concatenate([[test+'_stat', test+'_pval', test+'_r1', test+'_r2'] for test in tests]))\n",
    "    df.index = lags\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def fit_sigmoid(ydata, xdata):\n",
    "    p0 = [max(ydata), np.median(xdata),1,min(ydata)] # this is an mandatory initial guess\n",
    "    popt, pcov = curve_fit(sigmoid, xdata, ydata,p0, method='dogbox')\n",
    "    \n",
    "    return popt, pcov\n",
    "\n",
    "def get_sigmoid(y, x):\n",
    "    popt, pcov = fit_sigmoid(y, x)\n",
    "    fitted = sigmoid(x, *popt)\n",
    "    return x, fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all instabilities\n",
    "\n",
    "def save_instabilities_to_disk(inpath, outpath, subfile='data/lcn_subjects.csv'):\n",
    "    fnames = glob(inpath+'*.csv')\n",
    "\n",
    "    all_instabilities = []\n",
    "    for fname in tqdm(fnames):\n",
    "        all_instabilities.append([fname]+list(compute_instabilities_from_ts(pd.read_csv(fname, header=None))))\n",
    "    df = pd.DataFrame(all_instabilities)\n",
    "\n",
    "    df['subject'] = df[0].apply(lambda x: int(x.split('/')[-1].split('_')[0].split('-')[1]))\n",
    "    df['ses']     = df[0].apply(lambda x: x.split('/')[-1].split('_')[1].split('-')[1])\n",
    "    df['task']    = df[0].apply(lambda x: x.split('/')[-1].split('_')[2].split('-')[1])\n",
    "    df['run']     = df[0].apply(lambda x: int(x.split('/')[-1].split('_')[3].split('-')[1].replace('.csv', '')))\n",
    "\n",
    "    subjs = pd.read_csv(subfile)\n",
    "    merged = pd.merge(df, subjs[['subject', 'age']], on='subject')\n",
    "    merged.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarsen(source_img, dest_img):\n",
    "    resampled   = image.resample_to_img(source_img, dest_img, 'nearest')\n",
    "\n",
    "    dest_data   = dest_img.get_fdata()\n",
    "    labels      = set(dest_data.flatten()).difference({0})\n",
    "\n",
    "    masks       = {label: image.new_img_like(\\\n",
    "                    ref_niimg=dest_img,\\\n",
    "                    data=dest_data==label,\\\n",
    "                    affine=dest_img.affine) for label in labels}\n",
    "\n",
    "    result_data = dest_data.copy()\n",
    "    total       = abs(resampled.get_fdata()).sum()\n",
    "    activations = []\n",
    "\n",
    "    for label in labels:\n",
    "        masked \t= masking.apply_mask(resampled, masks[label])\n",
    "        #1 normalize by total activation of source image\n",
    "#         val = np.sum(abs(masked))/total \n",
    "        #2 normalize by number of voxels in the dest image\n",
    "        val = np.mean(masking.apply_mask(resampled, masks[label])) \n",
    "        \n",
    "        activations.append(val)\n",
    "        result_data[result_data==label] = val\n",
    "    \n",
    "    result_img    = image.new_img_like(dest_img, result_data, dest_img.affine)\n",
    "    \n",
    "    results       = pd.Series(activations)\n",
    "    results.index = labels\n",
    "    results.index = results.index.astype(int)\n",
    "    return results, result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_instabilities_from_ts_subnetworks(time_series, fname, rois, nonoverlapping=True, win_len=30):\n",
    "    chunks = []\n",
    "    steps = range(len(time_series))\n",
    "    steps = steps[::win_len] if nonoverlapping else steps\n",
    "    for step in steps:\n",
    "        chunk = time_series[step:step+win_len]\n",
    "        if len(chunk)<win_len:\n",
    "            continue\n",
    "        chunks.append(chunk.values)\n",
    "\n",
    "    conn = ConnectivityMeasure(kind='correlation')\n",
    "    corrs = conn.fit_transform(chunks)\n",
    "    corrs = [pd.DataFrame(corr) for corr in corrs]\n",
    "\n",
    "    networks = rois['functional_network'].unique()\n",
    "    instabilities = []\n",
    "    for network in networks:\n",
    "        idcs = np.where(rois['functional_network']==network)[0]\n",
    "        tau1 = [norm(corrs[idx].loc[idcs, idcs]-corrs[idx-1].loc[idcs, idcs])/len(idcs)\\\n",
    "                         for idx in range(1, len(corrs))]\n",
    "        instabilities.append([fname, network]+tau1)\n",
    "    return instabilities\n",
    "\n",
    "def save_instabilities_to_disk_subnetworks(fnames, outpath, rois, subfile='data/lcn_subjects.csv'):\n",
    "\n",
    "    all_instabilities = []\n",
    "    for fname in tqdm(fnames):\n",
    "        ts = pd.read_csv(fname, header=None)\n",
    "        all_instabilities = all_instabilities + \\\n",
    "            compute_instabilities_from_ts_subnetworks(ts, fname, rois=rois)\n",
    "    df = pd.DataFrame(all_instabilities)\n",
    "    \n",
    "    df['subject'] = df[0].apply(lambda x: int(x.split('/')[-1].split('_')[0].split('-')[1]))\n",
    "    df['ses']     = df[0].apply(lambda x: x.split('/')[-1].split('_')[1].split('-')[1])\n",
    "    df['task']    = df[0].apply(lambda x: x.split('/')[-1].split('_')[2].split('-')[1])\n",
    "    df['run']     = df[0].apply(lambda x: int(x.split('/')[-1].split('_')[3].split('-')[1].replace('.csv', '')))\n",
    "\n",
    "    subjs = pd.read_csv(subfile)\n",
    "    merged = pd.merge(df, subjs[['subject', 'age']], on='subject')\n",
    "    merged.to_csv(outpath)\n",
    "    \n",
    "    \n",
    "def save_instabilities_to_disk_subnetworks_biobank(fnames, outpath, rois, subfile='out/age_diabetes.csv'):\n",
    "    all_instabilities = []\n",
    "    for fname in tqdm(fnames):\n",
    "        ts = pd.read_csv(fname, header=None)\n",
    "        all_instabilities = all_instabilities + \\\n",
    "            compute_instabilities_from_ts_subnetworks(ts, fname, rois=rois)\n",
    "    df = pd.DataFrame(all_instabilities)\n",
    "    df.index = df[0].apply(lambda x: int(x.split('/')[-1].replace('.csv', '')))\n",
    "    subjs = pd.read_csv(subfile, index_col=0)\n",
    "    merged = pd.merge(df, subjs, left_index=True, right_index=True)\n",
    "    merged.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fontsize(small=8, medium=10, large=12):\n",
    "    plt.rc('font', size=small)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=small)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=medium)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=small)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=small)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=small)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=large)  # fontsize of the figure title\n",
    "    \n",
    "def sigmoid_plot(toplot, color='black', desc=None, fontsize=20, ax=None, fit_sigmoid=True):\n",
    "        \n",
    "    ax.tick_params(axis='x', which='both', rotation=90)\n",
    "    ax.tick_params(axis='y', which='both')\n",
    "    \n",
    "    ax.errorbar(toplot.index, y=toplot['mean'], yerr=toplot['sem'], marker='o', fmt='.', color=color);\n",
    "    ax.set_ylabel('Instability');\n",
    "    ax.set_xlabel('Age in YEARS');\n",
    "    ax.set_xticks(toplot.index);\n",
    "    ax.set_xticklabels([\"%s (n=%s)\" % (idx, int(x['n'])) for idx, x in toplot.iterrows()]);\n",
    "    if desc is not None:\n",
    "        ax.set_title('%s (N=%s)' % (desc, int(toplot['n'].sum())))\n",
    "\n",
    "    if fit_sigmoid:\n",
    "        x, y = get_sigmoid(toplot['mean'].values, toplot.index)\n",
    "        ax.plot(x, y, color=color, linewidth=2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}